\section{Linear Algebra III}

\subsection{Matrix Inversion}

\begin{itemize}
    \item For a scalar $a$, the inverse: $a^{-1} = \frac{1}{a}$, where $a^{-1}a = 1$.
    \item We can also invert a matrix $A$: $A^{-1}A = AA^{-1} = \text{I}$
    \begin{itemize}
        \item Note: if $A^{-1}$ does not exist, then $A$ is singular/not invertible.
        \item $A^T \neq A^{-1}$
    \end{itemize}
    \item For a $2 \times 2$ matrix, we use the following steps. We'll cover larger matrices after, for which we'll use a lengthier, but more intuitive process.
    \begin{align*}
        A & = \begin{bmatrix}
            a & b \\
            c & d 
        \end{bmatrix} \\
        A^{-1} & = \frac{1}{ad - bc} 
        \begin{bmatrix}
            d & -b \\
            -c & a 
        \end{bmatrix}
    \end{align*}

    Where $\frac{1}{ad - bc}$ is the \emph{determinant}, which we discuss more later. An example:

    \begin{align*}
        A & = \begin{bmatrix}
            1 & 2 \\
            3 & 4 
        \end{bmatrix} \\
        A^{-1} & = \frac{1}{(1\cdot4) - (3 \cdot 2)} 
        \begin{bmatrix}
            4 & -2 \\
            -3 & 1 
        \end{bmatrix} \\
        & = \frac{1}{-2} 
        \begin{bmatrix}
            4 & -2 \\
            -3 & 1
        \end{bmatrix} = 
        \begin{bmatrix}
            -2 & 1 \\
            3/2 & -\frac{1}{2}
        \end{bmatrix}
    \end{align*}
    
    \item How to check if $A^{-1}A = \text{I}$?
    \begin{equation*}
        \begin{bmatrix}
            -2 & 1 \\
            3/2 & -1/2
        \end{bmatrix}
        \begin{bmatrix}
            1 & 2 \\
            3 & 4
        \end{bmatrix}
        = 
        \begin{bmatrix}
            1 & 0 \\
            0 & 1
        \end{bmatrix}
    \end{equation*}
\end{itemize}

\subsection{Properties of Matrix Inversion}

Singular vs nonsingular:

\begin{itemize}
    \item Invertible = Nonsingular
    \item Noninvertible = Singular
    \item If $A$ is nonsingular, then $A^{-1}$ is nonsingular
    \item If $A$ \& $B$ are nonsingular, then $(AB)^{-1} = B^{-1}A^{-1}$
    \item If $A$ is nonsingular, then $(A^T)^{-1} = (A^{-1})^T$
\end{itemize}

\noindent Some conditions for nonsingularity (we can find $A^{-1}$, there are often more conditions listed, but they are technically covered by these three):

\begin{enumerate}
    \item Rows and columns are linearly independent
    \begin{itemize}
        \item No rows and/or columns add up to each other. Generally speaking if rows and columns are not linearly independent, then the matrix is invertible.
    \end{itemize}
    \item Matrix $A$ is row equivalent to $I$ (can we use row operators to turn $A$ into the identity matrix?)
    % \item $n \times n$ matrix $A$ is of rank $n$ (number of linearly independent rows and columns, equivalent to condition 1)
    \item The determinant (we cover below) is not 0.
    %\item $\forall$ (``for all'') vectors $b$, $\exists !$ (there exists a unique solution) to $Ax = b$ 
\end{enumerate}

\noindent One intuitive and practical procedure for finding $A^{-1}$, regardless of the size:

\begin{enumerate}
    \item Find $[A|\text{I}]$ where both $A$ and I are both $n \times n$
    \item Find the reduced row echelon form for A (left side)
    \item If step 2 gives us $[\text{I}|C]$, then $C = A^{-1}$
\end{enumerate}

\noindent A note on rank: If $A$ is $m \times n$, then the rank($A$) = 
\[ \text{min} \begin{cases} 
    \text{max \# of linearly ind. rows} \\
    \text{max \# of linearly ind. cols} 
\end{cases}
\]

\begin{itemize}
    \item E.g.: 
    $\begin{bmatrix}
        \bm{1} & \bm{2} & \bm{3} \\
        \bm{1} & 2 & 3 \\
        \bm{2} & 4 & 6
    \end{bmatrix}$ Only the bold rows and columns are linearly ind. So, rank$(A) =$ min$(1,1) = 1$
\end{itemize}

\begin{itemize}
    \item \textbf{Examples:}

    $
    \bm{A}\text{I} = 
    \begin{bmatrix}[ccc|ccc]
        1 & 1 & 1 & 1 & 0 & 0 \\
        0 & 2 & 3 & 0 & 1 & 0 \\
        5 & 5 & 1 & 0 & 0 & 1
    \end{bmatrix}
    $

    \begin{itemize}
        \item Subtract 1/2 r2 from r1, divide r2 by 2, subtract 5r1 from r3
    \end{itemize}

    $
    = \begin{bmatrix}[ccc|ccc]
        1 & 0 & -1/2 & 1 & - 1/2 & 0 \\
        0 & 1 & 3/2 & 0 & 1/2 & 0 \\
        0 & 0 &  4 & -5 & 0 & 1
    \end{bmatrix}
    $

    \begin{itemize}
        \item Subtract 1/8r3 from r1, add 3/8r3 to r2, divide r3 by -4
    \end{itemize}

    $
    = \begin{bmatrix}[ccc|ccc]
        1 & 0 & 0 & 13/8 & -1/2 & -1/8 \\
        0 & 1 & 0 & 15/8 & 1/2 & 3/8 \\
        0 & 0 & 1 & 5/4 & 0 & -1/4 
    \end{bmatrix}
    $

    \begin{itemize}
        \item Where the right-hand side matrix is $A^{-1}$
    \end{itemize}

    \item Another example, but $A^{-1}$ doesn't exist:

    $\begin{bmatrix}[ccc|ccc]
        1 & 2 & -3 & 1 & 0 & 0 \\
        1 & -2 & 1 & 0 & 1 & 0 \\
        5 & -2 & -3 & 0 & 0 & 1
    \end{bmatrix}$

    \begin{itemize}
        \item Subtract r1 from r2, subtract 5r1 from r3
    \end{itemize}

    $\begin{bmatrix}
        1 & 2 & 3 & ... & ... & ... \\
        0 & -4 & 4 & ... & ... & ... \\
        0 & -12 & 12 & ... & ... & ...
    \end{bmatrix}$

    \begin{itemize}
        \item We aren't concerned with the right-hand side matrix because the rows of the left matrix are not independent. r3 is a linear combination of r2, meaning the matrix is singular... 
    \end{itemize}

\end{itemize}

\subsection{Determinant}

\begin{itemize}
    \item Determinants convert a matrix into a scalar but can only be defined for a square matrix. Determinants are useful for checking if a matrix is invertible. They also can play a role in solving for systems of equations. The formula is straightforward for a $2 \times 2$ matrix, but less so for larger matrices. 
    \item Let 
    $A = 
    \begin{vmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22}
    \end{vmatrix}
    \begin{bmatrix}
        a_{11} & a_{12} \\
        a_{21} & a_{22}
    \end{bmatrix}$
    \begin{itemize}
        \item The determinant of $A$ is the two diagonal products differenced:
        
        $|A| = (a_{11} \cdot a_{22}) - (a_{21} \cdot a_{12})$
    \end{itemize}
    \item Examples:
    \begin{itemize}
        \item $\begin{bmatrix}
            2 & 3 \\ 
            1 & 5 
        \end{bmatrix} 
        \Rightarrow 
        (2 \cdot 5) - (3 \cdot 1) = 7$
        \item  $\begin{bmatrix}
            1 & 1 \\
            2 & 2 
        \end{bmatrix} 
        \Rightarrow 
        2 - 2  = 0$
    \end{itemize}

    \item For a $3 \times 3$ matrix we sum the products of all elements in any row or column, alternating signs, and the determinants of a specific $2 \times 2$ submatrix. An element's submatrix is the remaining elements when the elements from the relevant row and column are removed. I.e., for:
    
    $A = \begin{bmatrix}
        a_{11} & a_{12} & a_{13} \\
        a_{21} & a_{22} & a_{23} \\
        a_{31} & a_{32} & a_{33} 
    \end{bmatrix}$

    The submatrix for $a_{23}$ is 
    $\begin{bmatrix}
        a_{11} & a_{12} \\
        a_{31} & a_{32}    
    \end{bmatrix}$ 

    Taking the first column, the determinant of $A$ is:

    $
    a_{11}
    \begin{vmatrix}
        a_{22} & a_{23} \\
        a_{32} & a_{33}    
    \end{vmatrix}
    - a_{21}
    \begin{vmatrix}
        a_{12} & a_{13} \\
        a_{32} & a_{33}
    \end{vmatrix}
    + a_{31}
    \begin{vmatrix}
        a_{12} & a_{13} \\
        a_{22} & a_{23}
    \end{vmatrix}
    $

    \item We can use any row or column. But it is best to use one with zeros, if available. 

    \item The \textbf{minor} of an element is the determinant of its submatrix.
    \begin{itemize}
        \item $M_{12} = 
        \begin{vmatrix}
            a_{21} & a_{23} \\
            a_{31} & a_{33}
        \end{vmatrix}
        =
        (a_{21} \cdot a_{33}) - (a_{31} \cdot a_{23})
        $
    \end{itemize}
\end{itemize}

\subsubsection{Determinants via Cofactor Expansion}

\begin{itemize}
    \item The \textbf{cofactor} of any element $i,j$: $C_{ij} = (-1)^{i + j}M_{ij}$, which is used for calculating the determinants of $n \times n$ matrices where $n > 2$.  $i$ is rows, $j$ is columns.
    \begin{itemize}
        \item $
        A = \begin{bmatrix}
            a_{11} & a_{12} & a_{13} \\
            a_{21} & a_{22} & a_{23} \\
            a_{31} & a_{32} & a_{33}
        \end{bmatrix} 
        $
        \item Ex: $C_{11} = (-1)^{1 + 1} \, \, M_{11} = 1\begin{vmatrix}
            a_{22} & a_{23} \\
            a_{32} & a_{33}
        \end{vmatrix}$
    \end{itemize} 
    \item The determinant of a $n \times n$ matrix where $n >2$ is the sum of the products of each element and its cofactor for any row or column. We just choose a single row or a single column -- ideally one with as many zeros as possible.
    \begin{itemize}
        \item Given row $i$:
        \begin{equation*}
            \text{det}(A) = \sum_{j = 1}^n a_{ij}C_{ij}
        \end{equation*}
        \item Or row $j$:
        \begin{equation*}
            \text{det}(A) = \sum_{i = 1}^n a_{ij}C_{ij}
        \end{equation*}
    \end{itemize}
    \item Let's show what this means:
    \item Choose the row or column with the most zeros:
    \begin{itemize}
        \item Ex: $\begin{bmatrix}
            1 & 2 & -3 & 4 \\
            -4 & 2 & 1 & 3 \\
            0 & 0 & 0 & -3 \\
            2 & 0 & -2 & 3
        \end{bmatrix}$
        \item So let's take all elements in row 3, multiply each times its cofactor, and add it all together. Because of the zeros, we only have to find one cofactor! 
        
        \begin{align*}
            |A| = \sum_{j=1}^4 a_{3j}C_{3j} & = a_{31}C_{31} + a_{32}C_{32} + a_{33}C_{33} + a_{34}C_{34} \\
            a_{34}C_{34} & = 0 + 0 + 0 + a_{34}C_{34} \\
            a_{34}C_{34} & = -3\left[(-1)^{3+4}
            \begin{vmatrix}
                1 & 2 & -3 \\
                -4 & 2 & 1 \\
                2 & 0 & 2
            \end{vmatrix}  
            \right] \\ 
            & = -3 \cdot -1 \left[2 
            \begin{vmatrix}
                2 & -3 \\
                2 & 1    
            \end{vmatrix} - 0 
            \begin{bmatrix}
                1 & -3 \\
                -4 & 1
            \end{bmatrix} + (-2)
            \begin{vmatrix}
                1 & 2 \\
                -4 & 2
            \end{vmatrix}
            \right] \\
            & = 3 \left[2(2 + 6) - 2(2+8) \right] \\
            & = 3[16 - 20] = 3(-4) = -12
        \end{align*}
    \end{itemize}
\end{itemize}

\begin{itemize}
    \item To wrap up, some useful properties of determinants:
    \begin{enumerate}
        \item $|A| = |A^T|$
        \item If a row or column of A is a linear combination of other rows or columns, then $\text{det}(A) = 0$
        \item If $A$ is diagonal, then $|A|$ is the product of the diagonals.
        \item $|AB| = |A| \cdot |B|$
        \item If $A$ is non-singular, then $|A^{-1}| = \frac{1}{|A|}$
    \end{enumerate}
\end{itemize}

\subsection{Adjoint Matrix}

\begin{itemize}
    \item \textbf{Adjoint matrix}: the transpose of a matrix of the cofactors of each element:
    
    \begin{equation*}
        \text{adj}(A) = 
        \begin{bmatrix}
            c_{11} & c_{12} & ...  & c_{1n} \\
            c_{21} & ... & ... & c_{2n} \\
            \vdots & & & \\
            c_{n1} & ... & ...  & c_{nn}     
        \end{bmatrix}
    \end{equation*}

    Where $c$ is an element's cofactor. For example:

    $
    A = \begin{bmatrix}
        1 & 2 & 3 \\
        4 & 5 & 6 \\
        7 & 8 & 9
    \end{bmatrix}
    \Rightarrow 
    C = 
    \begin{bmatrix}
        + \begin{vmatrix}
            5 & 6 \\
            8 & 9
        \end{vmatrix} & 
        - \begin{vmatrix}
            4 & 6 \\
            7 & 9 
        \end{vmatrix} & 
        + \begin{vmatrix}
            4 & 5 \\
            7 & 8
        \end{vmatrix} \\ \\
        - \begin{vmatrix}
            2 & 3 \\
            8 & 9 
        \end{vmatrix} &
        + \begin{vmatrix}
            1 & 3 \\
            7 & 9
        \end{vmatrix} & 
        - \begin{vmatrix}
            1 & 2 \\
            7 & 8
        \end{vmatrix} \\ \\ 
        + \begin{vmatrix}
            2 & 3 \\
            5 & 6
        \end{vmatrix} & 
        - \begin{vmatrix}
            1 & 3 \\
            4 & 6
        \end{vmatrix} & 
        + \begin{vmatrix}
            1 & 2 \\
            4 & 5
        \end{vmatrix}
    \end{bmatrix}
    = 
    \begin{bmatrix}
        -3 & 6 & -3 \\
        6 & -12 & 6 \\
        -3 & 6 & -3
    \end{bmatrix}
    $

    Where the last matrix is the adjoint matrix. 
    
    This information gives us another way to calculate the inverse of $A$ with the following formula: 

    \begin{equation*}
        A^{-1} = \frac{\text{adj}^T}{|A|}
    \end{equation*}

\end{itemize}



\subsection{Trace}

Let's close with something simpler. The \textbf{trace} of a $n \times n$ matrix is just the sum of the diagonal elements.

\begin{equation*}
    \text{Tr} (A) = \Sigma_{i=1}^n a_{11} = a_{11} + a_{22} + ... + a_{nn}
\end{equation*}

\noindent This is less frequently used, but worth being aware of.